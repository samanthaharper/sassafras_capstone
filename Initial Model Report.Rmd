---
title: 'Initial Model Report: Sassafras'
author: "Samantha Harper"
date: "24 November 2024"
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
    toc_float: yes
    theme: flatly
    fig_crop: no
subtitle: Capstone
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE, 
                      cache.comments = TRUE,
                      size = 13)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Turn off scientific notation
options(scipen=999)

# Set seed; add CMRG for parallelization
set.seed(50009, "L'Ecuyer-CMRG")

# Clean, set up, and load
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
rm(list = ls(all = TRUE))

pacman::p_load(tidyverse, 
               ggplot2, 
               kableExtra,
               BiocManager,
               DESeq2,
               RColorBrewer,
               gridExtra,
               ggrepel,
               e1071,
               caret,
               randomForest,
               ranger,
               multtest,
               nestedcv,
               Rfast
)
```

# Background

*I haven't seen any obvious need to adjust these so far, but I could be missing something*

## Research Question

The research question is: Can we use machine learning algorithms to mine SNPs to find gene or gene regions of interest between natural cultivars (strains) of Sassafras?

## Hypothesis

Hypothesis: Underlying genes, as identified by SNPs, in Sassafras are influenced by environmental factors because environmental pressure can cause mutations to persist in a population that is unique to each area.

## Prediction

Prediction: Populations of Sassafras that are under high environmental pressure are more likely to have many predictive SNPs due to evolutionary influences. 

# Introduction

*first draft of introduction for final paper*

* Importance of Sassafras
* Details of original paper and findings
* Goals for research: identify genes of interest using SNPs
* Future research implications (climate change, controlled studies, selective breeding, genetic modifications)

# Methods

The plan proposed last week was to create a out-of-box random forest as a preliminary model. The random forest model was chosen as the first model to try because of it's relative resistance to overfitting and it's more transparent nature. This makes it easier to compare to other models and the findings of the original study. 

K-fold cross validation was also proposed as a method to apply. K-fold Cross validation is the process of slipping the test set into k 'folds' where all but one fold are used to train the model, and the final fold is used to evaluate. The broad range of accuracy obtained by training the data on a number of unique sets, and evaluating on k unique sets, is used to predict the efficacy of the model on unseen data. 

* Explain assumptions and testing

# Testing
```{r, echo=FALSE}
doSplits <- function(vst, algorithm, splitRatio, filterCutoff) {
  ### @vst = vst dataset as extracted from DESeq2
  ### @algorithm = ML algorithm used; currently set up for rf and svm
  ### @splitRatio = the split ratio to employ (training size)
  ### @filterCutoff = the filter cutoff for median number of VST gene counts
  
  ## According to the Valabas et al. (2019) paper, make sure that we are filtering in TRAINING set only! 

  # Extract the VST data and transpose
  tVST <- t(assay(vst))
  
  # We do have gene names, e.g., TRNAV-UAC that are malformed for ranger and randomForest. We will fix that before proceeding:
  for (c in 1:ncol(tVST)) {
    colName <- colnames(tVST)[c]
    colName <- gsub("-", "_", colName)
    colName -> colnames(tVST)[c]
  }
  
  ## Add the metadata as columns & merge
  df1 <- cbind(colData(vst)[1], colData(vst)[3], colData(vst)[2])       ## We don't need the size factors
  tVST <- merge(tVST, df1, by = "row.names")

  ## The merge turns back into a dataframe and removes the sample names from the rows; let's put them back:
  rownames(tVST) <- tVST[,1]
  tVST <- tVST[,-1]
  
  if(algorithm == "svm") {
    ## Make the factors unordered
    tVST <- tVST %>% 
      mutate_if(is.ordered, factor, ordered = FALSE)
  }
  
  ## Create the data partitions
  ind <- createDataPartition(y = tVST[, c("Treatment")],     ## Treatment is evenly distributed
                             p = splitRatio,                    ## % into training
                             list = FALSE)                      ## don't return a list
  train <- tVST[ind, ]
  test <- tVST[-ind,]
  
  ## Now apply the filtering:
  # Calculate row medians of VST gene counts
  medians <- rowMedians(assay(vst))

  # Filter the features out of train:
  train <- train[, medians > filterCutoff]  
  print(paste0("After filtering, the number of genes remaining in the dataset are: ", ncol(train)))

  splits <- list(train, test)
  return(splits)
}
```

```{r, echo = FALSE}
findOverlappingGenes <- function(lfc, important) {
  ### @lfc = the log-fold change cutoff you'd like to employ on the originall DESeq results
  ### @important = the list, df, or matrix that contains the importance values from the ML classifier; make sure it is already filtered if needed.

  res <- resultsDESeq %>% 
    as.data.frame() %>% 
    filter(abs(log2FoldChange) >= lfc)   # Make sure to filter by the ABSOLUTE VALUE :)
  
  # Move the rownames (genes) back to a column
  res$geneID <- rownames(res)
  # Coerce to a dataframe, if needed
  important <- important %>% 
    as.data.frame() %>% 
    filter()
  # Move the rownames (genes) back to a column, if needed
  if (!"geneID" %in% colnames(important)) {
      important$geneID <- rownames(important)
  }
  #Perform an inner join to find the overlap
  overlap <- inner_join(res, important, by = "geneID")
  
  return(overlap)
}
```

```{r, echo=FALSE}
compareConfusion <- function(confusionList) {
  ## instantiate
  finalDF <- data.frame()
  for(i in 1:length(confusionList)) {
    ## The first one
    if(i == 1) {
      confMat <- confusionList[[i]]   ## grab the first one
      df <- confMat$overall %>% as.data.frame() 
      finalDF <- rownames(df) %>% as.data.frame()
      colnames(finalDF)[1] <- "Metric"
      finalDF$`Confusion Matrix 1`  <- df[, 1]       ## grab the value
    }
    if(i > 1) {
      name <- paste0('Confusion Matrix ', i)
      confMat <- confusionList[[i]]
      df <- confMat$overall %>% as.data.frame()
      finalDF[, name] <- df[, 1]       ## grab the value
    }
  }
  return(finalDF)
}
```

```{r}
# Load in data
load("vst_all_timepoints.Rdata")
```

```{r}
splits <- doSplits(vst = vsData, algorithm = "rf", splitRatio = 0.8, filterCutoff = 5)
train <- splits[[1]]
test <- splits[[2]]
```
Error I keep getting below:

Error in h(simpleError(msg, call)) :
error in evaluating the argument 'x' in selecting a method for function 't': unable to find an inherited method for function ‘elementMetadata’ for signature ‘"SimpleList"’


## Algorithm 

## Assumptions

## Overfitting

# Discussion

* key takeaways and revised plan (refer to plan)

* Next steps for model tuning and selection
* Additional models and validation
* How to tune hyperparameters
* How did the initial model change plans?

# Appendix

* data dictionary

https://github.com/samanthaharper/sassafras_capstone

