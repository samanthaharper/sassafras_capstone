---
title: 'Data Preprocessing and Feature Engineering: Sassafras'
author: "Samantha Harper"
date: "20 November 2024"
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
    toc_float: yes
    theme: flatly
    fig_crop: no
subtitle: Capstone
---
*Note: Model Assignment at bottom of file*

# Background

## Research Question

The research question is: Can we use machine learning algorithms to mine SNPs to find gene or gene regions of interest between natural cultivars (strains) of Sassafras?

## Hypothesis

Hypothesis: Underlying genes, as identified by SNPs, in Sassafras are influenced by environmental factors because environmental pressure can cause mutations to persist in a population that is unique to each area.

## Prediction

Prediction: Populations of Sassafras that are under high environmental pressure are more likely to have many predictive SNPs due to evolutionary influences. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE, 
                      cache.comments = TRUE,
                      size = 13)
```

# Methods
The goals for data pre-processing and feature engineering were to transform the VCF data into a format compatible with machine learning algorithms and to apply some level of feature selection to prepare for accurate results from the ML models. The first step is to transform the dataset out of VCF format, keeping only the relevant data. Then normalization or centering should be applied to the data. The data needs to be split into test and training sets, and feature selection should remove lowly expressed genes only after the data has been split. Then, cross validation can be used to help with hyperparameter tuning and with our relatively small sample size. 

## Preprocessing
The vcfR package was used to convert the VCF data to a tibble. The metadata from the original study was then cleaned and joined with the sample names, since the original dataset only contained a row for each population, instead of for each sample. The DeSeq Object requires a very specific matrix, so the metadata and the genetic data both had row names set and compared to ensure the object would load correctly. Once the DeSeq Object was created, a Variation Stabilizing Transformation was applied. Since the read counts for genetic data vary wildly, it is essential that some kind of transformation be applied. The 'local' VST was applied after comparing the four options and comparing the resulting standard deviation and spread of the data. 

## Feature Selection
Before any feature selection, the data was split into test and training sets to prevent data leakage. Principal Component Analysis (PCA) was applied to the data. PCA is used to reduce the dimensionality of the data and therefore help highlight important features. The PCA strategies that were used didn't seem to fit the numerical nature of the data, and an alternative will be considered before modelling. 

### Results

```{r}
#Install packages and load library

pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
rm(list = ls(all = TRUE))

pacman::p_load(tidyverse,
               vcfR,
               ggplot2, 
               kableExtra,
               BiocManager,
               DESeq2,
               pheatmap,
               RColorBrewer,
               vsn,
               gridExtra,
               ggrepel,
               e1071,
               caret,
               randomForest,
               ranger,
               multtest,
               janitor
)
```

```{r}
#Load VCF data
vcf <- read.vcfR( "Data/SNP.vcf", verbose = FALSE )
#examine VCF object
vcf
```



```{r}
#Convert data to tibble
data <- vcfR2tidy(vcf)
```

```{r}
#Isolate Counts Data
gdata <- data$gt
#Load Metadata
sass_metadata <- read_csv("Data/sass_metadata.csv", col_types = cols(Longitude = col_number(), Latitude = col_number(), `Number of Samples` = col_integer(), Temperature = col_number(), Precipitation = col_integer(), `Altitude (m)` = col_integer()))
```


```{r}
#the goal is to have my matrix match the counts data from demo1
#then I can feed the matrix into DeSeq2 and use the resulting counts
#and hopefully those are correct?
gdata$chrom_info <- paste("S", gdata$ChromKey, "_", gdata$POS)
gdata$chrom_info <- gsub(" ", "", gdata$chrom_info)
gdata <- gdata %>% select(chrom_info, Indiv, gt_AD)
gdata <- gdata %>% pivot_wider(names_from = Indiv, values_from = gt_AD)
```

```{r}
#set up matrix
gdata <- as.data.frame(gdata)
rownames(gdata) <- gdata$chrom_info
gdata <- gdata[ ,-1]
```


```{r}
#create proper metadata
indivs <- unique(data$gt$Indiv)
indivs <- as.data.frame(indivs)
indivs$id <- substr(indivs$indivs, 1, 2)
sass_metadata$id <- substr(sass_metadata$Code, 1, 2)
joined_meta <- inner_join(sass_metadata, indivs, by = 'id')
joined_meta <- subset(joined_meta, select = -c(Code))
```


```{r}
#Join metadata with sample names
joined_meta <- as.data.frame(joined_meta)
rownames(joined_meta) <- joined_meta$indivs
joined_meta <- subset(joined_meta, select = -c(indivs))
```

```{r}
#rename column to exclude the space
joined_meta$Altitude <- joined_meta$`Altitude (m)`
```



```{r, echo = TRUE, results='asis'}
#Compare columns and rows to prep for DeSeq Object
print(all(colnames(gdata) %in% rownames(joined_meta)))
print(all(colnames(gdata) == rownames(joined_meta)))
```



```{r, message=FALSE, warning=FALSE, results='hide'}
#Create Design Object
dsgnObject <- DESeqDataSetFromMatrix(countData = gdata, 
                                     colData = joined_meta,
                                     design = ~ Longitude + Latitude + Temperature + Altitude)
dim(dsgnObject)
```

```{r, echo = F, warning=FALSE, message=FALSE}
#Set up VST
runVST <- function(dsgnObject, blind, fitType, makePlot = TRUE, writeTable = FALSE, writeRData = FALSE) {
  ## Perform the VST
  
  # Check if the fitType is the regularized log:
  if(fitType == "rlog") {
    vsData <- rlog(dsgnObject, blind = blind)
  }
  ## Otherwise:
  else {
    vsData <- varianceStabilizingTransformation(dsgnObject, 
                                              blind = blind, 
                                              fitType = fitType)
  }
  
  if(makePlot == TRUE) {
    # Plot the effect of the VS transform:
    p1 <- meanSdPlot(assay(dsgnObject), plot = F)
    p1 <- p1$gg + ggtitle("Before Variance Stabilization") + 
      scale_fill_gradient(low = "cadetblue", high = "purple") + 
      theme_bw() + theme(legend.position = "bottom")
    p2 <- meanSdPlot(assay(vsData), plot = F)
    p2 <- p2$gg + ggtitle("After Variance Stabilization") + 
      scale_fill_gradient(low = "cadetblue", high = "purple") + 
      theme_bw() + theme(legend.position = "bottom")
    grid.arrange(p1, p2, nrow=1)
  }
  
  if(writeTable == TRUE) {
    # Write the data for future use, if needed:
    write.table(assay(vsData),
              file = "vst.txt",
              sep="\t", 
              quote=F, 
              row.names=T)
  }
  if(writeRData == TRUE) {
    save(vsData, file="vst_all_timepoints.Rdata")
  }
  return(vsData)
}
#VST 1
#runVST(dsgnObject, blind = FALSE, fitType = "parametric", makePlot = TRUE, writeTable = FALSE, writeRData = FALSE)
```

```{r, echo=FALSE}
#Examine Log-Log Plot
meanCounts <- rowMeans(assay(dsgnObject))      ## Per locus, what is the average expression
varCounts <- apply(assay(dsgnObject), 1, var)  ## Apply the variance function by margin = 1, which is rows

plot(log(varCounts) ~ log(meanCounts), 
     ylab = "Natural-log Variance in Gene Expression", 
     xlab = "Natural-log Mean Expression", 
     main = "\nLog-Log plot of variance by mean for each gene\n should be approximately linear.\n", 
     pch = 16, 
     cex = 0.75)
abline(lm(log(varCounts+0.0001) ~ log((meanCounts+0.0001))), 
       col = "#a8325e", 
       lty = 2, 
       lwd = 2)
```
We will use the local VST function because it has the lowest standard deviation. 

```{r}
#VST 2
runVST(dsgnObject, blind = FALSE, fitType = "local", makePlot = TRUE, writeTable = FALSE, writeRData = TRUE)

#VST 3
#runVST(dsgnObject, blind = FALSE, fitType = "mean", makePlot = TRUE, writeTable = FALSE, writeRData = FALSE)

#VST 4
#runVST(dsgnObject, blind = FALSE, fitType = "rlog", makePlot = TRUE, writeTable = FALSE, writeRData = FALSE)

```
```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
#Split data

#Do Splits from Project 1
#So we don't have to reinvent the wheel here
doSplits <- function(vst, algorithm, splitRatio, filterCutoff) {
  ### @vst = vst dataset as extracted from DESeq2
  ### @algorithm = ML algorithm used; currently set up for rf and svm
  ### @splitRatio = the split ratio to employ (training size)
  ### @filterCutoff = the filter cutoff for median number of VST gene counts
  
  ## According to the Valabas et al. (2019) paper, make sure that we are filtering in TRAINING set only! 

  # Extract the VST data and transpose
  tVST <- t(assay(vst))
  
  # We do have gene names, e.g., TRNAV-UAC that are malformed for ranger and randomForest. We will fix that before proceeding:
  for (c in 1:ncol(tVST)) {
    colName <- colnames(tVST)[c]
    colName <- gsub("-", "_", colName)
    colName -> colnames(tVST)[c]
  }
  
  ## Add the metadata as columns & merge
  df1 <- cbind(colData(dsgnObject)[1], colData(dsgnObject)[2], colData(dsgnObject)[4], colData(dsgnObject)[5], colData(dsgnObject)[7])       ## We don't need the size factors
  tVST <- merge(tVST, df1, by = "row.names")

  ## The merge turns back into a dataframe and removes the sample names from the rows; let's put them back:
  rownames(tVST) <- tVST[,1]
  tVST <- tVST[,-1]
  
  if(algorithm == "svm") {
    ## Make the factors unordered
    tVST <- tVST %>% 
      mutate_if(is.ordered, factor, ordered = FALSE)
  }
  
  ## Create the data partitions
  ind <- createDataPartition(y = tVST[, c("id")],     ## Treatment is evenly distributed
                             p = splitRatio,                    ## % into training
                             list = FALSE)                      ## don't return a list
  train <- tVST[ind, ]
  test <- tVST[-ind,]
  
  ## Now apply the filtering:
  # Calculate row medians of VST gene counts
  medians <- rowMedians(assay(vst))

  # Filter the features out of train:
  train <- train[, medians > filterCutoff]  
  print(paste0("After filtering, the number of genes remaining in the dataset are: ", ncol(train)))

  splits <- list(train, test)
  return(splits)
}
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Load in data
load("vst_all_timepoints.Rdata")

#Split data
#Do before processing to avoid data leakage - adjusted splitRatio to allow for adequate data in the test set
splits <- doSplits(vst = vsData, algorithm = "rf", splitRatio = 0.8, filterCutoff = 5)
train <- splits[[1]]
test <- splits[[2]]
```


```{r, echo = FALSE, warning=FALSE, message=FALSE, fig.width = 8}
#Create a PCA data frame
#I don't think this works as well with numerical data (as opposed to the original categorical variables)
pca <- plotPCA(vsData, 
               intgroup = c("Longitude", "Latitude", "Temperature", "Precipitation", "Altitude", "id"), 
               returnData = TRUE,
               ntop = 500)
percentVar <- round(100 * attr(pca, "percentVar"))

#Plot the PCA with the % variance attributable to PC1 and PC2
ggplot(pca, aes(PC1, PC2, color = Temperature)) +
  geom_point(size=3, alpha = 0.85) +
  labs(x = paste0("PC1: ",percentVar[1], "% variance"), 
       y = paste0("PC2: ",percentVar[2], "% variance"),
       title = "PCA of Gene Expression by Samples") +
  xlim(-30,30) +
  ylim(-15, 15) +
  theme_bw() +
  theme(axis.text = element_text(size = 13),
        legend.position = "right") +
  facet_wrap(~Longitude, nrow = 1)
```

```{r}
#Install packages and load library
if (!requireNamespace("caret", quietly = TRUE)) {
  install.packages("caret")
}
library(caret)
```


## Discussion

While genetic data is harder to work with, the data preparation has worked fairly well and given some very interesting and important takeaways. Importantly, the variables being considered are all numerical, which lends itself to different machine learning and feature engineering approaches than mainly categorical data. It is clear that the data benefited greatly from the variance stabilizing transformation, as the raw counts had a standard deviation of over 200, while the VST data was just over two, and with a much more stable spread. Principal component analysis, at least the method I used, was not particularly useful and may not be the best choice going forward. Additional feature engineering may be required depending on the type of machine learning model and how well the models perform. It is clear that there are impacts of environment on the SNPs found in each population, however the exact details are still to come. 

## Next Steps

The next step is to run some out-of-box machine learning models and evaluate my choices so far. A random forest model, a support vector machine, and a neural-network will all be trained on the training data and tested on the test set. After the oob models are tested, other techniques can be used to tune the hyperparameters of each model and improve model accuracy. These techniques can include cross validation, grid search, or elastic net. 


# Initial Model Report

*I normally keep these separate, but for some unknown reason, my splits function wouldn't work in my other file. Sorry!*

## Background

*I haven't seen any obvious need to adjust these so far, but I could be missing something*

### Research Question

The research question is: Can we use machine learning algorithms to mine SNPs to find gene or gene regions of interest between natural cultivars (strains) of Sassafras?

### Hypothesis

Hypothesis: Underlying genes, as identified by SNPs, in Sassafras are influenced by environmental factors because environmental pressure can cause mutations to persist in a population that is unique to each area.

### Prediction

Prediction: Populations of Sassafras that are under high environmental pressure are more likely to have many predictive SNPs due to evolutionary influences. 

## Introduction

*first draft of introduction for final paper*

* Importance of Sassafras
* Details of original paper and findings
* Goals for research: identify genes of interest using SNPs
* Future research implications (climate change, controlled studies, selective breeding, genetic modifications)

## Methods

The plan proposed last week was to create a out-of-box random forest as a preliminary model. The random forest model was chosen as the first model to try because of it's relative resistance to overfitting and it's more transparent nature. This makes it easier to compare to other models and the findings of the original study. 

K-fold cross validation was also proposed as a method to apply. K-fold Cross validation is the process of slipping the test set into k 'folds' where all but one fold are used to train the model, and the final fold is used to evaluate. The broad range of accuracy obtained by training the data on a number of unique sets, and evaluating on k unique sets, is used to predict the efficacy of the model on unseen data. 

* Explain assumptions and testing

## Model

```{r, echo = TRUE}
train$id = factor(train$id) 

rfOOB <- randomForest::randomForest(
  id ~ ., 
  data = train)

#Confusion matrix works for classification trees, but this is a regression problem
rfOOB$confusion %>% 
  kable(
    format = "html",
    caption = "Table 4. Results of the OOB Random Forest") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F))

```



```{r, echo = FALSE}
pred.test.rf <- predict(rfOOB, test)
```

```{r, echo = FALSE}
pred.train.rf <- predict(rfOOB, train, type = 'response')
confMat <- confusionMatrix(train$id,
                pred.train.rf)

overall <- round(confMat$overall, 3)
data.frame(overall) %>% kable(
    format = "html",
    caption = "Table 5. OOB RF Train - Results of Confusion Matrix") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F)
)
```
```{r, echo = FALSE}
test$id <- factor(test$id) 

pred.test.rf <- predict(rfOOB, test, type = "response")

confMat <- confusionMatrix(test$id, pred.test.rf)

overall <- round(confMat$overall, 3)
data.frame(overall) %>% kable(
    format = "html",
    caption = "Table 6. OOB RF Test - Results of Confusion Matrix") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F)
)
```

* The accuracy for the training set is 1! That's great!
* But the accuracy for the test set is only .63
* That suggest that we have been overfitting
* Now we can focus on CV and tuning the hyperparameters

```{r, echo=FALSE, message = FALSE, warning = FALSE}

importantRF <- rfOOB$importance     ## Store the important genes!

importantRF %>% 
  as.data.frame() %>% 
  arrange(desc(MeanDecreaseGini)) %>% 
  top_n(10) %>% 
  kable(
    format = "html",
    caption = "Table 7. Top 10 important genes identiftied by the OOB Random Forest") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F))
```
* Here we can isolate the important genes that are related to each population

## SVM

```{r, echo=TRUE, warning=FALSE, message=FALSE}
svmOOB <- svm(id ~ ., 
  data = train,
  kernel = "linear",
  na.action = na.omit
)
```

```{r, echo = FALSE}
paste0("The total number of support vectors was: ", svmOOB$tot.nSV)
```

```{r, echo = FALSE}
pred.train.svm <- predict(svmOOB, train, type = 'response')
confMat <- confusionMatrix(train$id,
                pred.train.svm)

overall <- round(confMat$overall, 3)
data.frame(overall) %>% kable(
    format = "html",
    caption = "Table 8. OOB SVM Train - Results of Confusion Matrix") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F)
)
```

```{r, echo = FALSE}
test$id <- factor(test$id) 

pred.test.svm <- predict(svmOOB, test, type = "response")
confMat <- confusionMatrix(test$id, pred.test.svm)

overall <- round(confMat$overall, 3)
data.frame(overall) %>% kable(
    format = "html",
    caption = "Table 9. OOB SVM Test - Results of Confusion Matrix") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F)
)
```

* The accuracy is lower for the testing set than for the training set (again, showing overfitting)
* But the accuracy is higher for the SVM than for the random forest

(Not sure why I'm doing wrong here)
```{r, echo=FALSE, message=FALSE, warning=FALSE}
labels <- train$id

tTrain <- train %>% 
  select(id) %>%
  t() %>% 
  as.matrix()

tscores <- mt.teststat(tTrain, 
                       labels, 
                       test = "t")
```

## Store the important genes!

```{r, echo = FALSE}
geneID <- rownames(tTrain)
importantsvm_1 <- data.frame(geneID, tscores)

importantsvm_1 %>% 
  arrange(desc(tscores)) %>% 
  top_n(10) %>% 
  kable(
    format = "html",
    caption = "Table 10. Top 10 important genes identiftied by the OOB SVM") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F))
```

## Assumptions

The assumptions for a random forest are feature independence and independent errors. However, neither random forest nor SVM requires any specific distribution for the data. 

## Tuning

```{r, echo = TRUE, eval=T}
rfOOB <- randomForest::randomForest(
  id ~ ., 
  data = train)

pred.test.rf <- predict(rfOOB, test, type = "response")
confMat <- confusionMatrix(test$id, pred.test.rf)
```

```{r, eval = T}
# Set the CV arguments
kFoldCtrl <- trainControl(method = "cv",    # for k-fold CV
                          number = 10)      # k
```

Fit the Random Forest model:
```{r}
rfCV <- train(id ~.,  
               data = train,
               method = "ranger",
               trControl = kFoldCtrl)    ## added in the 10-fold CV
```

```{r, echo = FALSE, warning=FALSE, message=FALSE, eval=T}
ggplot(rfCV, highlight = TRUE) +
  ggtitle("Random Forest Performance After 10-fold CV") + 
  theme_bw()
```

```{r, echo = F}
rfCV$bestTune %>% 
kable(
    format = "html",
    caption = "Table 1. Results of the 10-fold CV Random Forest") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F))
```

```{r, echo=TRUE}
rfCV <- train(id ~.,  
               data = train,
               method = "ranger",
               trControl = kFoldCtrl, 
               tuneGrid = rfCV$bestTune)   # Add in the results of the CV and auto tuning


pred.test.rf <- predict(rfCV, test, type = "raw")  ## type is now 'raw' 
# Store the confusion matrix
confMatCV <- confusionMatrix(test$id, pred.test.rf)
```

```{r, echo=FALSE}
compareConfusion <- function(confusionList) {
  ## instantiate
  finalDF <- data.frame()
  for(i in 1:length(confusionList)) {
    ## The first one
    if(i == 1) {
      confMat <- confusionList[[i]]   ## grab the first one
      df <- confMat$overall %>% as.data.frame() 
      finalDF <- rownames(df) %>% as.data.frame()
      colnames(finalDF)[1] <- "Metric"
      finalDF$`Confusion Matrix 1`  <- df[, 1]       ## grab the value
    }
    if(i > 1) {
      name <- paste0('Confusion Matrix ', i)
      confMat <- confusionList[[i]]
      df <- confMat$overall %>% as.data.frame()
      finalDF[, name] <- df[, 1]       ## grab the value
    }
  }
  return(finalDF)
}
```

```{r, echo = FALSE}
compareConfusion(confusionList = list(confMat, confMatCV)) %>% 
  kable(
    format = "html",
    caption = "Table 2. Comparing Accuracy - Random Forest") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F))
```
```{r, echo = FALSE}
# Make a set of k = 10 seeds for reproducibility
seeds <- vector(mode = "list", length = 11)
for(i in 1:10) {
  seeds[[i]]<- sample.int(n=1000, 54)   # Increase 54 if you have a larger grid! 
}
# For the last model
seeds[[11]]<-sample.int(1000, 1)
```

```{r, echo = FALSE}
# Set the CV arguments
kFoldCtrl <- trainControl(method = "cv",    # for k-fold CV
                          number = 10,      # k
                          seeds = seeds)    # sets the seeds, one for each split

```

```{r}
searchGrid <- expand.grid(
  mtry = floor(ncol(train) * c(.05, .15, .25, .35, .45)),
  splitrule = c("gini", "extratrees"),
  min.node.size = c(1, 3, 5, 10) 
)
```

```{r}
rfTuned <- train(id ~.,  
               data = train,
               method = "ranger",
               trControl = kFoldCtrl, 
               tuneGrid = searchGrid # Add in the search grid
)
```

```{r, echo = FALSE}
ggplot(rfTuned, highlight = TRUE) +
  ggtitle("Random Forest Performance Grid Search Tuning") + 
  theme_bw()
```
```{r}
rfTuned$bestTune %>% 
kable(
    format = "html",
    caption = "Table 3. Results of the Grid Search on Random Forest") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F))
```

```{r}
rfTuned <- train(id ~.,  
               data = train,
               method = "ranger",
               trControl = kFoldCtrl, 
               tuneGrid = rfTuned$bestTune)   # Add in the results of the CV and auto tuning
```


```{r}
pred.test.rf <- predict(rfTuned, test, type = "raw")  ## type is now 'raw' 
# Store the confusion matrix
confMatTuned <- confusionMatrix(test$id, pred.test.rf)
```

#### 6. Compare the three confusion matrices:
```{r}
compareConfusion(confusionList = list(confMat, confMatCV, confMatTuned)) %>% 
    kable(
    format = "html",
    caption = "Table 4. Comparing Accuracy - Random Forest") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F))
```
## Discussion

The biggest challenge is how to deal with the multiple numeric variables when using 'id' as the label. The adjustment to using the categorical variable 'id' (which represents the specific populations of the trees where samples were taken), has benefits and drawbacks. The code runs a lot better and many techniques that I was planning on usign work better, but I fear that some specificity will be lost. This may instead lead to several steps of research where genes are identified for each population and then further ML analyses identify which factors of each population is most influencial. 

Going forward, I will continue to tune the hyperparameters for the RF and the SVM model. The CV and search were both fairly basic and there are more advanced options I can approach. 
